<!doctype html><html lang=zh-CN dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="扫描版的PDF文档在进行阅读标记时存在局限，若不幸扫描PDF是一本没有目录的书籍，那阅读体验就更加糟糕了，就算使用acrobat手动创建书签，那也将会是一项复杂而繁琐的工作。


  PDF补丁丁可以一键创建目录书签，不过针对的是非扫描版PDF。
幸好，随着文档数字化和 AI 技术的发展，将扫描PDF 文档精准地转化为结构化EPUB/Markdown成为可能。
  pdf-craft 就提供了这种能力。"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://wangy325.github.io/zh-cn/posts/2026/20260111_pdf_craft_optiise_on_rtx4060/"><meta property="og:site_name" content="xf&pl"><meta property="og:title" content="在RTX4060上运行并调试pdf-craft"><meta property="og:description" content="扫描版的PDF文档在进行阅读标记时存在局限，若不幸扫描PDF是一本没有目录的书籍，那阅读体验就更加糟糕了，就算使用acrobat手动创建书签，那也将会是一项复杂而繁琐的工作。
PDF补丁丁可以一键创建目录书签，不过针对的是非扫描版PDF。
幸好，随着文档数字化和 AI 技术的发展，将扫描PDF 文档精准地转化为结构化EPUB/Markdown成为可能。 pdf-craft 就提供了这种能力。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-11T00:00:00+08:00"><meta property="article:modified_time" content="2026-01-13T23:12:19+08:00"><meta property="article:tag" content="VLLM"><title>在RTX4060上运行并调试pdf-craft | xf&amp;pl</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://wangy325.github.io/zh-cn/posts/2026/20260111_pdf_craft_optiise_on_rtx4060/><link rel=stylesheet href=/book.min.3cf95a1bd697b902b62eb0c72147591c0a33fb3b5f76dc80ccf10bfd26e5a29f.css integrity="sha256-PPlaG9aXuQK2LrDHIUdZHAoz+ztfdtyAzPEL/Sblop8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/zh-cn.search.min.dbf8ea71cb09507c2a85681839af2897f569ff8b860a636f89074ccfa4a0af44.js integrity="sha256-2/jqccsJUHwqhWgYOa8ol/Vp/4uGCmNviQdMz6Sgr0Q=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-HHNKR5H8KN"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HHNKR5H8KN")}</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><div class=navi-bar id=navibar><div id=page-title><h2 class=book-brand><a class="flex align-center" href=/zh-cn/><span>xf&amp;pl</span></a></h2></div><div class=page-navi-col><nav><ul><li><a href=/zh-cn/docs/java/>Java</a></li><li><a href=/zh-cn/docs/design_pattern/>DP</a></li><li class=book-section-flat><a href=/zh-cn/docs/note/>Mooc</a></li><li class=book-section-flat><a href=/zh-cn/docs/snippets/>Snippets</a></li></ul><ul><li><a href=/zh-cn/posts/>Blog</a></li><li><a href=/zh-cn/archive/>Archive</a></li></ul></nav></div><div class="book-search hidden"><input type=text id=book-search-input placeholder=搜索 aria-label=搜索 maxlength=64 data-hotkeys=s/ required pattern=\S+.*>
<span class=clear_search><img src=/svg/close.svg onclick=clearInput()></span><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><script>function clearInput(){document.getElementById("book-search-input").value="";for(var e=document.getElementById("book-search-results");e.firstChild;)e.removeChild(e.firstChild)}</script><ul class=book-languages><li><a class="flex align-center"><img src=/svg/language.svg class=book-icon alt=Languages>
简体中文</a></label><ul><li><a href=https://wangy325.github.io/en/>English</a></li></ul></li></ul></div><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>在RTX4060上运行并调试pdf-craft</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><div id=toc-new><ul class=nav><li class=nav-item><a id=t安装 href=javascript:void(0) onclick='return scrolly("安装"),!1'>安装</a><ul class=nav><li class=nav-item><a id=t创建并激活虚拟环境 href=javascript:void(0) onclick='return scrolly("创建并激活虚拟环境"),!1'>创建并激活虚拟环境</a></li><li class=nav-item><a id=t正确安装pytorch href=javascript:void(0) onclick='return scrolly("正确安装pytorch"),!1'>正确安装pytorch</a></li><li class=nav-item><a id=t安装pdfcraft href=javascript:void(0) onclick='return scrolly("安装pdf-craft"),!1'>安装pdf-craft</a></li><li class=nav-item><a id=t安装poppler href=javascript:void(0) onclick='return scrolly("安装poppler"),!1'>安装poppler</a></li><li class=nav-item><a id=t检查安装 href=javascript:void(0) onclick='return scrolly("检查安装"),!1'>检查安装</a></li><li class=nav-item><a id=t首次运行 href=javascript:void(0) onclick='return scrolly("首次运行"),!1'>首次运行</a></li></ul></li><li class=nav-item><a id=t优化 href=javascript:void(0) onclick='return scrolly("优化"),!1'>优化</a><ul class=nav><li class=nav-item><a id=t开启flashattn href=javascript:void(0) onclick='return scrolly("开启flash-attn"),!1'>开启flash-attn</a></li><li class=nav-item><a id=t调整参数 href=javascript:void(0) onclick='return scrolly("调整参数"),!1'>调整参数</a></li><li class=nav-item><a id=t使用4bit量化 href=javascript:void(0) onclick='return scrolly("使用4bit量化"),!1'>使用4bit量化</a></li></ul></li><li class=nav-item><a id=t后记 href=javascript:void(0) onclick='return scrolly("后记"),!1'>后记</a></li></ul></div><script>function scrolly(e){const t=40;var n=document.getElementById(e).offsetTop;window.scrollTo({top:n+t,behaviour:"smooth"})}</script></aside></header><article class="markdown book-post"><h1>在RTX4060上运行并调试pdf-craft</h1><div class="postmeta flex align-cneter" style=justify-content:center><div class="created flex align-cneter"><img src=/svg/calendar-event.svg class=book-icon alt>
<a title='创建者 wangy325'>2026-01-11</a></div><div class="cates flex align-center"><img src=/svg/cates.svg class=book-icon alt>
<a href=/zh-cn/categories/python/>Python</a></div><div class="tags flex align-center"><img src=/svg/tag-alt.svg class=book-icon alt>
<a href=/zh-cn/tags/vLLM/>VLLM</a></div><div class="last-modified flex align center"><img src=/svg/edit.svg class=book-icon alt>
<a title='最后修改者 wangy | 2026-01-13'>2026-01-13</a></div></div><p>扫描版的PDF文档在进行阅读标记时存在局限，若不幸扫描PDF是一本没有目录的书籍，那阅读体验就更加糟糕了，就算使用acrobat手动创建书签，那也将会是一项复杂而繁琐的工作。</p><blockquote><p><a href=https://github.com/wmjordan/PDFPatcher>PDF补丁丁</a>可以一键创建目录书签，不过针对的是非扫描版PDF。</p></blockquote><p>幸好，随着文档数字化和 AI 技术的发展，将扫描PDF 文档精准地转化为结构化EPUB/Markdown成为可能。
<a href=https://github.com/oomol-lab/pdf-craft>pdf-craft</a> 就提供了这种能力。</p><blockquote><p><em><code>pdf-craft</code>基于
<a href=https://github.com/deepseek-ai/DeepSeek-OCR>DeepSeek OCR</a> 进行文档识别。支持表格、公式等复杂内容的识别。通过 GPU 加速，<code>pdf-craft</code>能够在本地完成从 PDF 到 Markdown 或 EPUB 的完整转换流程。转换过程中，<code>pdf-craft</code> 会自动识别文档结构，准确提取正文内容，同时过滤页眉、页脚等干扰信息。对于包含脚注、公式、表格的学术或技术文档，<code>pdf-craft</code>也能妥善处理，保留这些重要元素（包括脚注中的图片等资源）。转换为 EPUB 时会自动生成目录。最终生成的 Markdown 或 EPUB 文件保持了原书的内容完整性和可读性。</em></p></blockquote><p>虽然结合了强大的DeepSeek-OCR视觉模型，提供了极高的识别准确度。然而，对于拥有 RTX 4060（8GB 显存）的 Windows 用户来说，如何在有限的显存下流畅运行并实现快速推理，需要进行一系列的配置优化。本文记录了从安装运行到调优的全过程。</p><h2 id=安装>安装
<a class=anchor href=#%e5%ae%89%e8%a3%85>#</a></h2><blockquote><p><code>pdf-craft</code>支持<code>3.10.x~ 3.13.x</code>。开始前请先检查python版本。</p></blockquote><h3 id=创建并激活虚拟环境>创建并激活虚拟环境
<a class=anchor href=#%e5%88%9b%e5%bb%ba%e5%b9%b6%e6%bf%80%e6%b4%bb%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>
</span></span><span class=line><span class=ln>2</span><span class=cl>mkdir pdf-craft  <span class=o>&amp;&amp;</span> <span class=nb>cd</span> pdf-craft
</span></span><span class=line><span class=ln>3</span><span class=cl><span class=c1>## 创建虚拟环境</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>python -m venv .venv
</span></span><span class=line><span class=ln>5</span><span class=cl>
</span></span><span class=line><span class=ln>6</span><span class=cl><span class=c1>## 激活虚拟环境</span>
</span></span><span class=line><span class=ln>7</span><span class=cl>.venv<span class=se>\S</span>cripts<span class=se>\a</span>ctivate
</span></span></code></pre></div><h3 id=正确安装pytorch>正确安装pytorch
<a class=anchor href=#%e6%ad%a3%e7%a1%ae%e5%ae%89%e8%a3%85pytorch>#</a></h3><ul><li><p>检查 CUDA 版本：在命令行输入 <code>nvidia-smi</code>，查看右上角的 CUDA Version。</p></li><li><p>获取安装命令：前往
<a href=https://pytorch.org/get-started/locally/>PyTorch 官网</a>。</p><blockquote><p>PyTorch的CUDA版本并不一定和系统的版本一致，一般不要高于系统版本。</p></blockquote></li><li><p>根据环境选择：选择 Stable -> Windows -> Pip -> Python -> CUDA 12.1（或对应的版本）。</p></li><li><p>安装pytorch：</p><p><code>pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128</code></p></li></ul><p>安装过程可能会花费一点时间，取决与你的网速。</p><p>如果发现下载速度慢，可尝试使用加速镜像:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>pip config <span class=nb>set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
</span></span></code></pre></div><h3 id=安装pdf-craft>安装pdf-craft
<a class=anchor href=#%e5%ae%89%e8%a3%85pdf-craft>#</a></h3><p><code>pip install pdf-craft</code></p><h3 id=安装poppler>安装poppler
<a class=anchor href=#%e5%ae%89%e8%a3%85poppler>#</a></h3><p><code>pdf-craft</code>依赖 <code>poppler</code> 处理 PDF 渲染。在 Windows 上需要手动配置：</p><p>从
<a href=https://github.com/oschwartz10612/poppler-windows/releases>poppler-windows releases</a>下载最新版本的二进制压缩包。</p><p>将其解压到您的工具目录（例如 C:\Program Files\poppler）。</p><p>配置环境变量：将解压目录下的 bin 文件夹路径（例如 C:\Program Files\poppler\Library\bin）添加到系统的 环境变量 PATH 中。</p><h3 id=检查安装>检查安装
<a class=anchor href=#%e6%a3%80%e6%9f%a5%e5%ae%89%e8%a3%85>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl><span class=c1>## 检查CUDA</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>python -c <span class=s2>&#34;import torch; print(&#39;CUDA 可用:&#39;, torch.cuda.is_available())&#34;</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>
</span></span><span class=line><span class=ln>4</span><span class=cl><span class=c1>## 2. 检查 poppler 是否识别成功</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>pdfinfo -v
</span></span><span class=line><span class=ln>6</span><span class=cl>
</span></span><span class=line><span class=ln>7</span><span class=cl><span class=c1>## 3. 验证模块导入</span>
</span></span><span class=line><span class=ln>8</span><span class=cl>python -c <span class=s2>&#34;import pdf_craft; print(&#39;pdf-craft 模块加载成功&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=首次运行>首次运行
<a class=anchor href=#%e9%a6%96%e6%ac%a1%e8%bf%90%e8%a1%8c>#</a></h3><p>以上操作都完成后，你可以使用IDE打开<code>pdf-craft</code>项目。这个项目是空的，目前仅仅安装好了依赖，是“可运行”状态，还需要一个脚本让其运行。</p><p>同时还需要一个源pdf，随便有找一个扫描版的pdf即可，测试阶段可以使用acrobat只截取一部分页面。完成后将其放在项目文件夹里，命名随意，假如就叫<code>input.pdf</code>好了。</p><p>在项目里创建一个<code>demo.py</code>并使用如下代码快速开始：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=ln> 1</span><span class=cl><span class=kn>from</span> <span class=nn>pdf_craft</span> <span class=kn>import</span> <span class=n>transform_epub</span><span class=p>,</span> <span class=n>BookMeta</span><span class=p>,</span> <span class=n>TableRender</span><span class=p>,</span> <span class=n>LaTeXRender</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=n>transform_epub</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=n>pdf_path</span><span class=o>=</span><span class=s2>&#34;input.pdf&#34;</span><span class=p>,</span> <span class=c1>##源pdf名</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>    <span class=n>epub_path</span><span class=o>=</span><span class=s2>&#34;output.epub&#34;</span><span class=p>,</span> <span class=c1>## 输出文件名</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>    <span class=n>analysing_path</span><span class=o>=</span><span class=s2>&#34;temp&#34;</span><span class=p>,</span>  <span class=c1>## 可选：指定临时文件夹</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=n>ocr_size</span><span class=o>=</span><span class=s2>&#34;base&#34;</span><span class=p>,</span>  <span class=c1>## 可选：tiny, small, base, large, gundam</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>    <span class=n>models_cache_path</span><span class=o>=</span><span class=s2>&#34;models&#34;</span><span class=p>,</span>  <span class=c1>## 可选：默认的模型下载路径</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>,</span>  <span class=c1>## 可选：渲染 PDF 页面的 DPI（默认：300）</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>    <span class=n>max_page_image_file_size</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>  <span class=c1>## 可选：最大图像文件大小（字节），超出时自动调整 DPI</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>    <span class=n>includes_cover</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1>## 可选：包含封面</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>    <span class=n>includes_footnotes</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1>## 可选：包含脚注</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=n>ignore_pdf_errors</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1>## 可选：遇到 PDF 渲染错误时继续处理</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=n>ignore_ocr_errors</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1>## 可选：遇到 OCR 识别错误时继续处理</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=n>generate_plot</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1>## 可选：生成可视化图表</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=n>toc_assumed</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1>## 可选：假设 PDF 包含目录页</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>    <span class=n>book_meta</span><span class=o>=</span><span class=n>BookMeta</span><span class=p>(</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>        <span class=n>title</span><span class=o>=</span><span class=s2>&#34;书名&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>        <span class=n>authors</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;作者1&#34;</span><span class=p>,</span> <span class=s2>&#34;作者2&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>        <span class=n>publisher</span><span class=o>=</span><span class=s2>&#34;出版社&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=ln>22</span><span class=cl>    <span class=n>lan</span><span class=o>=</span><span class=s2>&#34;zh&#34;</span><span class=p>,</span>  <span class=c1>## 可选：语言 (zh/en)</span>
</span></span><span class=line><span class=ln>23</span><span class=cl>    <span class=n>table_render</span><span class=o>=</span><span class=n>TableRender</span><span class=o>.</span><span class=n>HTML</span><span class=p>,</span>  <span class=c1>## 可选：表格渲染方式</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>    <span class=n>latex_render</span><span class=o>=</span><span class=n>LaTeXRender</span><span class=o>.</span><span class=n>MATHML</span><span class=p>,</span>  <span class=c1>## 可选：公式渲染方式</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>    <span class=n>inline_latex</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1>## 可选：保留内联 LaTeX 表达式</span>
</span></span><span class=line><span class=ln>26</span><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>不出意外的话，运行脚本，就会自动下载模型。然后进行本地推理了。模型会下载在项目的<code>models</code>文件夹里。如上面的<code>models_cache_path</code>配置的那样。</p><blockquote><p>RTX4060在首次以默认配置运行时，使用11张带封面的扫描PDF跑出了20分钟的好成绩。这显然是不能接受的。按照这个速度，跑一本书（按300页算）的话，可能需要10个小时不止！</p><p>看到这个数据，顿时不想折腾了。</p></blockquote><p>好在考虑到4060的AI性能，提前咨询AI是否存在可优化的空间。AI倒是给出了一些指导意见。Gemini对于模型速度的预测还是过于乐观了，竟然说可以达到2s处理一张PDF，一本300页的书籍可能只需要10分钟！不过正是由于这份乐观，此文才有后续。</p><h2 id=优化>优化
<a class=anchor href=#%e4%bc%98%e5%8c%96>#</a></h2><h3 id=开启flash-attn>开启flash-attn
<a class=anchor href=#%e5%bc%80%e5%90%afflash-attn>#</a></h3><p>通过<code>pip install</code>安装<code>flash-attention</code>大概率会失败，直接
<a href=https://github.com/Dao-AILab/flash-attention/releases>下载</a>已经编译好的对应本地（cuda、pytorch、python）版本<code>.whl</code>文件，再手动安装，会省事得多。</p><p>安装命令为<code>pip install 'xxx.whl'</code>。</p><p>到这里，模型下载和基本的优化就已经完成了。可以使用测试脚本来验证：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=ln> 1</span><span class=cl><span class=kn>import</span> <span class=nn>importlib.util</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>
</span></span><span class=line><span class=ln> 4</span><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Torch version: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CUDA available: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CUDA device: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_name</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;BF16 supported: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_bf16_supported</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=n>flash_attn</span> <span class=o>=</span> <span class=n>importlib</span><span class=o>.</span><span class=n>util</span><span class=o>.</span><span class=n>find_spec</span><span class=p>(</span><span class=s2>&#34;flash_attn&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>11</span><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Flash Attention installed: </span><span class=si>{</span><span class=n>flash_attn</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>
</span></span><span class=line><span class=ln>13</span><span class=cl><span class=c1>###### output:</span>
</span></span><span class=line><span class=ln>14</span><span class=cl><span class=c1>## Torch version: 2.9.1+cu128</span>
</span></span><span class=line><span class=ln>15</span><span class=cl><span class=c1>## CUDA available: True</span>
</span></span><span class=line><span class=ln>16</span><span class=cl><span class=c1>## CUDA device: NVIDIA GeForce RTX 4060</span>
</span></span><span class=line><span class=ln>17</span><span class=cl><span class=c1>## BF16 supported: True</span>
</span></span><span class=line><span class=ln>18</span><span class=cl><span class=c1>## Flash Attention installed: True</span>
</span></span></code></pre></div><h3 id=调整参数>调整参数
<a class=anchor href=#%e8%b0%83%e6%95%b4%e5%8f%82%e6%95%b0>#</a></h3><p>上面的实例可以看到，还是提供了不少参数用来控制模型效率：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=ln>1</span><span class=cl><span class=n>ocr_size</span><span class=o>=</span><span class=s2>&#34;base&#34;</span><span class=p>,</span>  <span class=c1>## 可选：tiny, small, base, large, gundam</span>
</span></span><span class=line><span class=ln>2</span><span class=cl><span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>,</span>  <span class=c1>## 可选：渲染 PDF 页面的 DPI（默认：300）</span>
</span></span><span class=line><span class=ln>3</span><span class=cl><span class=n>max_page_image_file_size</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>  <span class=c1>## 可选：最大图像文件大小（字节），超出时自动调整 DPI</span>
</span></span></code></pre></div><p>理论上<code>ocr_size</code>越小越快，相对地，OCR精度也会越低，例如会出现“6”识别为“8”的情形。<code>dpi</code>也是同理，不过对于一般的PDF文档，将<code>dpi</code>设置为144或者96是合理的。<code>max_page_image_file_size</code>这个参数就更加直观了。因为pdf-craft的第一步就是将利用popper将单页转换为图片，图片越小速度越看，同理精度也就越低。</p><p>以上参数的设置，一般设置为<code>base</code>和<code>144</code>可以达到效率和质量的平衡。</p><blockquote><p>不过实际测算下来，使用300dpi和144dpi的单页速度差不多。</p></blockquote><h3 id=使用4bit量化>使用4bit量化
<a class=anchor href=#%e4%bd%bf%e7%94%a84bit%e9%87%8f%e5%8c%96>#</a></h3><p>最值得一提的就是这个优化了，折腾了个大的，发现是负优化（🤡）。</p><p>由于一开始就当心pdf-craft在4060上OOM，还“未雨绸缪”了一番，与Gemini友好交流后，决定开启4bit量化，节省VRAM不爆显存才是最重要的呀！</p><p>安装完pdf-craft后，就开始研究源码里加载模型的部分，试图在加载模型的配置里作一点小小的patch(使用4bit量化)。</p><p>在<code>doc_page_extractor\model.py</code>里，定义了<code>DeepSeekOCRHugginfaceModel</code>，里面的<code>_ensure_models</code>方法定义了模型的加载：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl> <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>    <span class=n>pretrained_model_name_or_path</span><span class=o>=</span><span class=n>name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>    <span class=n>_attn_implementation</span><span class=o>=</span><span class=n>_ATTN_IMPLEMENTATION</span><span class=p>,</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>    <span class=n>use_safetensors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln>6</span><span class=cl>    <span class=n>cache_dir</span><span class=o>=</span><span class=n>cache_dir</span><span class=p>,</span>
</span></span><span class=line><span class=ln>7</span><span class=cl>    <span class=n>local_files_only</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_local_only</span><span class=p>,</span>
</span></span><span class=line><span class=ln>8</span><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=ln>9</span><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>(</span><span class=n>device_number</span><span class=p>)</span>
</span></span></code></pre></div><p>现在要做的就在初始化模型时，使用4bit量化。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=n>compute_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_bf16_supported</span><span class=p>()</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>float16</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>
</span></span><span class=line><span class=ln> 3</span><span class=cl><span class=n>quantization_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>    <span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>    <span class=n>bnb_4bit_use_double_quant</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>    <span class=n>bnb_4bit_quant_type</span><span class=o>=</span><span class=s1>&#39;nf4&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=n>bnb_4bit_compute_dtype</span><span class=o>=</span><span class=n>compute_dtype</span><span class=p>,</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;正在使用 4-bit NF4 量化加载模型，计算精度: </span><span class=si>{</span><span class=n>compute_dtype</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>10</span><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;flash-attention-2 是否开启: </span><span class=si>{</span><span class=n>_ATTN_IMPLEMENTATION</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=ln>11</span><span class=cl>
</span></span><span class=line><span class=ln>12</span><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>    <span class=n>pretrained_model_name_or_path</span><span class=o>=</span><span class=n>name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=n>_attn_implementation</span><span class=o>=</span><span class=n>_ATTN_IMPLEMENTATION</span><span class=p>,</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=n>use_safetensors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>    <span class=n>cache_dir</span><span class=o>=</span><span class=n>cache_dir</span><span class=p>,</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>    <span class=n>local_files_only</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_local_only</span><span class=p>,</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>    <span class=n>quantization_config</span><span class=o>=</span><span class=n>quantization_config</span><span class=p>,</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=ln>21</span><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>不过仅仅这样做的话会出现一个<em>datatype mismatch</em>的错误。原因在于这个配置仅仅指定了LLM的4bit量化datatype（bf16）,而deepseekOCR的视觉编码（vision encoder）使用的datatype还是（Float32）。不过不是什么大问题，antiGravity改改就好了😁。实际上就是在模型初始化侧强制设置模型的所有模块应用统一的数据类型。</p><blockquote><p>虽然如此看来很简单，修改应用4bit量化测试成功的过程还是花费了我大几个小时的时间~</p><p>原因在于antiGravity解决编程问题并不是一次成功的。哈哈。</p></blockquote><p>幸运的是，成功后，再次使用11张的测试pdf跑完，时间从缩短一半约为10分钟了。不过还是远远达不到“我所预料的Gemini给我的速度”。</p><p>有趣的是，我关闭4bit量化后，这个时间更短了，为4分钟左右了。这个速度就比较符合我的认知，开启<code>flash-attn</code>后速度会快2-3倍。不过，4bit量化是负优化了？？</p><p>以下是Gemini给出的答案：</p><blockquote><p>开启<code>load_in_4bit=True</code>反而更慢的原因是因为 Windows 下的<code>bitsandbytes</code>库回退到了极其低效的计算模式。</p><p>详细原因分析：</p><p>在之前的测试中，当开启<code>4-bit</code>量化时，Benchmark 脚本的报错信息中出现了以下关键堆栈：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=ln>1</span><span class=cl>File <span class=s2>&#34;...\bitsandbytes\backends\cuda\ops.py&#34;</span>, line 510, in _gemv_4bit_impl
</span></span><span class=line><span class=ln>2</span><span class=cl>lib.cgemm_4bit_inference_naive_bf16<span class=o>(</span>
</span></span></code></pre></div><p>这里 naive（朴素/简单）是核心线索。它表明：</p><p>缺失优化算子：标准的<code>bitsandbytes</code>库在 Linux 上有高度优化的 CUDA 内核（kernels），但在 Windows 上支持尚不完美。如果未能加载到针对你显卡架构编译的<code>4-bit</code>优化算子，它就会使用备用的 "naive" 实现。</p><p>性能差异：这个 "naive" 实现可能是串行的，或者是并未针对 GPU 并行计算进行优化的代码。相比之下，你的 RTX 4060 在运行标准的 BF16/FP16 精度时（即<code>load_in_4bit=False</code>），使用的是 NVIDIA 官方高度优化的 cuBLAS 库，速度极快。</p><p>结果：量化本应通过减少显存占用和访存带宽来加速，但在这种情况下，计算本身的低效完全抵消并严重拖累了整体速度（导致 3.75s 变成了 50s+）。</p></blockquote><p>黄梁一梦啊。</p><p>不过1分钟接近3页到4页的处理速度，应该也差不多到4060的极限了，目前来讲。</p><h2 id=后记>后记
<a class=anchor href=#%e5%90%8e%e8%ae%b0>#</a></h2><p>整体测试下来，deepseek-OCR的识别准确率还是不错的。不过对于内容的格式排版，甚至是基本的换行，有时候都不完美。</p><p>更坏的是，有很严重的内容缺失。</p><center style=font-size:.8rem;font-style:italic;color:grey><img alt=‘’ src=/img/2026-01-13215529.png width=100%><p>识别结果与原pdf对比（ocr_size:small, dpi:72）</center><p>上图可以看到，完全丢掉了第二页和第三页(仅仅保留了最后一句话，截图未显示)的内容。目前还不清楚类似这种情况的内容确实是否和<code>ocr_size</code>以及<code>dpi</code>配置有关。不过理论上应该是没什么关系的。</p><p>看来目前完全期待pdf-craft来“修复”扫描版本的PDF书籍还是不太可能的。毕竟除了格式上面的问题，内容的缺失确实还需要更多的实践测试才行。</p><p>不管怎样，它为更加“优雅”地阅读扫描版PDF提供了一个实际可行的操作。除了书籍之外，对于其他的短篇幅的诸如论文之类，无论是处理时间和人为校验起来，时间成本会更低，还是值得尝试的。</p></article><div class=next-prev><hr><div class="page-navi flex" id=pagenavi><div class=pre-page-none>←∅</div><div class=next-page><a href=https://wangy325.github.io/zh-cn/posts/2025/20250523_pattern_matching/>语法糖--模式匹配</a> →</div></div></div><section class=related><div class=rel-title>相似文章</div><div class=rel-items><a href=/zh-cn/posts/2025/20250427_spilt_markdown/ class=related__link><div>使用MarkdownIt库拆分Markdown文本</div></a><a href=/zh-cn/docs/snippets/pys/threading/15_threading_event/ class=related__link><div>线程-事件</div></a><a href=/zh-cn/docs/snippets/pys/threading/14_threading_semaphore/ class=related__link><div>线程-信号量</div></a></div><hr></section><div class=book-comments><div class=giscus-thread></div><script src=https://giscus.app/client.js data-repo=wangy325/wangy325.github.io data-repo-id=R_kgDOHhhDZg data-category=Announcements data-category-id=DIC_kwDOHhhDZs4ChbMn data-mapping=og:title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=noborder_light data-loading=lazy data-lang=zh-CN crossorigin=anonymous async></script></div><div class=copyright><hr class=cline><p>Notes and Memos by wangy325
<a href=https://creativecommons.org/licenses/by/4.0/deed.en title="Creative Commons Attribution">&copy; CC BY 4.0</a></p><p>2019-2026. Powered by
<a href=https://github.com/alex-shpak/hugo-book>Hugo Book</a>
Presented by GitHub.</p></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><div id=toc-new><ul class=nav><li class=nav-item><a id=t安装 href=javascript:void(0) onclick='return scrolly("安装"),!1'>安装</a><ul class=nav><li class=nav-item><a id=t创建并激活虚拟环境 href=javascript:void(0) onclick='return scrolly("创建并激活虚拟环境"),!1'>创建并激活虚拟环境</a></li><li class=nav-item><a id=t正确安装pytorch href=javascript:void(0) onclick='return scrolly("正确安装pytorch"),!1'>正确安装pytorch</a></li><li class=nav-item><a id=t安装pdfcraft href=javascript:void(0) onclick='return scrolly("安装pdf-craft"),!1'>安装pdf-craft</a></li><li class=nav-item><a id=t安装poppler href=javascript:void(0) onclick='return scrolly("安装poppler"),!1'>安装poppler</a></li><li class=nav-item><a id=t检查安装 href=javascript:void(0) onclick='return scrolly("检查安装"),!1'>检查安装</a></li><li class=nav-item><a id=t首次运行 href=javascript:void(0) onclick='return scrolly("首次运行"),!1'>首次运行</a></li></ul></li><li class=nav-item><a id=t优化 href=javascript:void(0) onclick='return scrolly("优化"),!1'>优化</a><ul class=nav><li class=nav-item><a id=t开启flashattn href=javascript:void(0) onclick='return scrolly("开启flash-attn"),!1'>开启flash-attn</a></li><li class=nav-item><a id=t调整参数 href=javascript:void(0) onclick='return scrolly("调整参数"),!1'>调整参数</a></li><li class=nav-item><a id=t使用4bit量化 href=javascript:void(0) onclick='return scrolly("使用4bit量化"),!1'>使用4bit量化</a></li></ul></li><li class=nav-item><a id=t后记 href=javascript:void(0) onclick='return scrolly("后记"),!1'>后记</a></li></ul></div><script>function scrolly(e){const t=40;var n=document.getElementById(e).offsetTop;window.scrollTo({top:n+t,behaviour:"smooth"})}</script></div></aside></main><div class=back-to-top><div id=back-to-top><img src=/svg/chevrons-up.svg srcset="/svg/chevrons-up.svg 500w" onclick=topFunction() class=black>
<img title=回到顶部 src=/svg/chevrons-up-blue.svg srcset="/svg/chevrons-up-blue.svg 500w" onclick=topFunction() class=blue></div><script>let mybutton=document.getElementById("back-to-top");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>500||document.documentElement.scrollTop>500?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){window.scrollTo({top:0,behavior:"smooth"})}</script></div><script src=/js/scroll-listening.js></script></body></html>